{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-classification-with-keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLKD90ueThfXbecCb9NxWp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs145442/nlp-projects-with-tf2/blob/master/sentiment_classification_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QahPG6gO-cKK"
      },
      "source": [
        "## 1. Gathering the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na3cvAoVpdDZ",
        "outputId": "4a1d71de-c6b1-4beb-badc-0c8ca0505f52"
      },
      "source": [
        "# add and unzip the dataset here\n",
        "! ls\n",
        "! wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
        "! unzip stanfordSentimentTreebank.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "--2020-11-21 11:42:09--  http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip [following]\n",
            "--2020-11-21 11:42:09--  https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6372817 (6.1M) [application/zip]\n",
            "Saving to: ‘stanfordSentimentTreebank.zip’\n",
            "\n",
            "stanfordSentimentTr 100%[===================>]   6.08M  11.9MB/s    in 0.5s    \n",
            "\n",
            "2020-11-21 11:42:10 (11.9 MB/s) - ‘stanfordSentimentTreebank.zip’ saved [6372817/6372817]\n",
            "\n",
            "Archive:  stanfordSentimentTreebank.zip\n",
            "   creating: stanfordSentimentTreebank/\n",
            "  inflating: stanfordSentimentTreebank/datasetSentences.txt  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/stanfordSentimentTreebank/\n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSentences.txt  \n",
            "  inflating: stanfordSentimentTreebank/datasetSplit.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSplit.txt  \n",
            "  inflating: stanfordSentimentTreebank/dictionary.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._dictionary.txt  \n",
            "  inflating: stanfordSentimentTreebank/original_rt_snippets.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._original_rt_snippets.txt  \n",
            "  inflating: stanfordSentimentTreebank/README.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._README.txt  \n",
            "  inflating: stanfordSentimentTreebank/sentiment_labels.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._sentiment_labels.txt  \n",
            "  inflating: stanfordSentimentTreebank/SOStr.txt  \n",
            "  inflating: stanfordSentimentTreebank/STree.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BHmfyEqqGm2",
        "outputId": "5e30dba7-5687-4b6f-b9ee-6ebdd3ffdf33"
      },
      "source": [
        "! cat stanfordSentimentTreebank/README.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stanford Sentiment Treebank V1.0\n",
            "\n",
            "This is the dataset of the paper:\n",
            "\n",
            "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\n",
            "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng and Christopher Potts\n",
            "Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)\n",
            "\n",
            "If you use this dataset in your research, please cite the above paper.\n",
            "\n",
            "@incollection{SocherEtAl2013:RNTN,\n",
            "title = {{Parsing With Compositional Vector Grammars}},\n",
            "author = {Richard Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher Manning and Andrew Ng and Christopher Potts},\n",
            "booktitle = {{EMNLP}},\n",
            "year = {2013}\n",
            "}\n",
            "\n",
            "This file includes:\n",
            "1. original_rt_snippets.txt contains 10,605 processed snippets from the original pool of Rotten Tomatoes HTML files. Please note that some snippet may contain multiple sentences.\n",
            "\n",
            "2. dictionary.txt contains all phrases and their IDs, separated by a vertical line |\n",
            "\n",
            "3. sentiment_labels.txt contains all phrase ids and the corresponding sentiment labels, separated by a vertical line.\n",
            "Note that you can recover the 5 classes by mapping the positivity probability using the following cut-offs:\n",
            "[0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\n",
            "for very negative, negative, neutral, positive, very positive, respectively.\n",
            "Please note that phrase ids and sentence ids are not the same.\n",
            "\n",
            "4. SOStr.txt and STree.txt encode the structure of the parse trees. \n",
            "STree encodes the trees in a parent pointer format. Each line corresponds to each sentence in the datasetSentences.txt file. The Matlab code of this paper will show you how to read this format if you are not familiar with it.\n",
            "\n",
            "5. datasetSentences.txt contains the sentence index, followed by the sentence string separated by a tab. These are the sentences of the train/dev/test sets.\n",
            "\n",
            "6. datasetSplit.txt contains the sentence index (corresponding to the index in datasetSentences.txt file) followed by the set label separated by a comma:\n",
            "\t1 = train\n",
            "\t2 = test\n",
            "\t3 = dev\n",
            "\n",
            "Please note that the datasetSentences.txt file has more sentences/lines than the original_rt_snippet.txt. \n",
            "Each row in the latter represents a snippet as shown on RT, whereas the former is each sub sentence as determined by the Stanford parser.\n",
            "\n",
            "For comparing research and training models, please use the provided train/dev/test splits.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSYcsx_dqLRE",
        "outputId": "c02d8e6b-e9d0-4a8c-efd1-596a76986640"
      },
      "source": [
        "# take a peek at the dataset format\n",
        "! echo \"----- contents of the treebank -------------------\"\n",
        "! ls stanfordSentimentTreebank\n",
        "! echo \"----- first 5 lines of dictionary.txt ------------\"\n",
        "! tail -n 5 stanfordSentimentTreebank/dictionary.txt\n",
        "! echo \"----- first 5 lines of sentiment_labels.txt ------\"\n",
        "! tail -n 5 stanfordSentimentTreebank/sentiment_labels.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- contents of the treebank -------------------\n",
            "datasetSentences.txt  dictionary.txt\t\tREADME.txt\t      SOStr.txt\n",
            "datasetSplit.txt      original_rt_snippets.txt\tsentiment_labels.txt  STree.txt\n",
            "----- first 5 lines of dictionary.txt ------------\n",
            "zoning ordinances to protect your community from the dullest science fiction|220441\n",
            "zzzzzzzzz|179256\n",
            "élan|220442\n",
            "É|220443\n",
            "É um passatempo descompromissado|220444\n",
            "----- first 5 lines of sentiment_labels.txt ------\n",
            "239227|0.36111\n",
            "239228|0.38889\n",
            "239229|0.33333\n",
            "239230|0.88889\n",
            "239231|0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-S8KuQIIjrB",
        "outputId": "393563fe-0c1b-4c46-cd86-571e11f323be"
      },
      "source": [
        "# install all the dependencies here\n",
        "! pip install bert-for-tf2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/820ccaf55f1e24b5dd43583ac0da6d86c2d27bbdfffadbba69bafe73ca93/bert-for-tf2-0.14.7.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 11.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.7-cp36-none-any.whl size=30539 sha256=93d63a88eb3b51ef88a2895e8c9370285c64967392593762033e24d3c642b272\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/f8/e2/b98f79a6b8cc898d8e4102b83acb8a098df7d27500a2bac912\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7304 sha256=191eb70e8532128f4468731a248e8204ec7cbf1c8c34ec8974b5bd89fc257136\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=b4ed562963300197b21686eb4a7338070981976e1ba3137e97c98cc29a45afc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.7 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM9xOwpSp7NF"
      },
      "source": [
        "# import all the dependencies here\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import bert\n",
        "\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "z-Nle8lFwEJL",
        "outputId": "d7985bc1-66ff-4c59-bb33-268facc1edbb"
      },
      "source": [
        "# reading the dataset\n",
        "dataset_df = pd.read_csv('stanfordSentimentTreebank/dictionary.txt', sep='\\n')\n",
        "dataset_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>!|0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>! '|22935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>! ''|18235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>! Alas|179257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>! Brilliant|22936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>! Brilliant !|40532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   !|0\n",
              "0            ! '|22935\n",
              "1           ! ''|18235\n",
              "2        ! Alas|179257\n",
              "3    ! Brilliant|22936\n",
              "4  ! Brilliant !|40532"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnLK9Th-xevI"
      },
      "source": [
        "# formatting the dataframe for processing\n",
        "dataset_df['phrase_text'] = dataset_df['!|0'].apply(lambda x: x.split('|')[0])\n",
        "dataset_df['phrase_ids'] = dataset_df['!|0'].apply(lambda x: x.split('|')[1])\n",
        "dataset_df = dataset_df.drop('!|0', axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qcmS89OHxpMT",
        "outputId": "8fc50da6-e5f6-4f10-f8bb-f8f58ba55fbd"
      },
      "source": [
        "# take a peek at the dataframe\n",
        "dataset_df.tail()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase_text</th>\n",
              "      <th>phrase_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>239226</th>\n",
              "      <td>zoning ordinances to protect your community fr...</td>\n",
              "      <td>220441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239227</th>\n",
              "      <td>zzzzzzzzz</td>\n",
              "      <td>179256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239228</th>\n",
              "      <td>élan</td>\n",
              "      <td>220442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239229</th>\n",
              "      <td>É</td>\n",
              "      <td>220443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239230</th>\n",
              "      <td>É um passatempo descompromissado</td>\n",
              "      <td>220444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              phrase_text phrase_ids\n",
              "239226  zoning ordinances to protect your community fr...     220441\n",
              "239227                                          zzzzzzzzz     179256\n",
              "239228                                               élan     220442\n",
              "239229                                                  É     220443\n",
              "239230                   É um passatempo descompromissado     220444"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "QgfEREDoycbU",
        "outputId": "13bf9c59-3dd6-402e-a597-2c3824bcb2a1"
      },
      "source": [
        "# reading the sentiment data\n",
        "sentiment_df = pd.read_csv('stanfordSentimentTreebank/sentiment_labels.txt', sep='\\n')\n",
        "sentiment_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase ids|sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0|0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1|0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2|0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3|0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4|0.42708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  phrase ids|sentiment values\n",
              "0                       0|0.5\n",
              "1                       1|0.5\n",
              "2                   2|0.44444\n",
              "3                       3|0.5\n",
              "4                   4|0.42708"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnxTPYOC4_FJ"
      },
      "source": [
        "# formatting the sentiment dataframe for processing\n",
        "sentiment_df['phrase_ids'] = sentiment_df['phrase ids|sentiment values'].apply(lambda x: x.split('|')[0])\n",
        "sentiment_df['sentiment_values'] = sentiment_df['phrase ids|sentiment values'].apply(lambda x: x.split('|')[1])\n",
        "sentiment_df = sentiment_df.drop('phrase ids|sentiment values', axis=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "9Ag4UWqo5P-T",
        "outputId": "ddc514bf-a91b-4a2a-fd33-053ec705a72f"
      },
      "source": [
        "sentiment_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase_ids</th>\n",
              "      <th>sentiment_values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.42708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  phrase_ids sentiment_values\n",
              "0          0              0.5\n",
              "1          1              0.5\n",
              "2          2          0.44444\n",
              "3          3              0.5\n",
              "4          4          0.42708"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr4PqXS45lqL",
        "outputId": "dc61e8ac-0fde-4f7a-bbf8-3ca49a11f9b9"
      },
      "source": [
        "# let's merge the phrases and sentiments\n",
        "dataset_sentiment_df = pd.merge(left=dataset_df, right=sentiment_df, how='inner', on='phrase_ids')\n",
        "# let's also validate the number of datapoints\n",
        "print(f\"dataset df shape: {dataset_df.shape}\")\n",
        "print(f\"sentiment df shape: {sentiment_df.shape}\")\n",
        "print(f\"dataset_sentiment df shape: {dataset_sentiment_df.shape}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset df shape: (239231, 2)\n",
            "sentiment df shape: (239232, 2)\n",
            "dataset_sentiment df shape: (239231, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BBvdrf58jIS"
      },
      "source": [
        "*seems good. we missed one datapoint while merging, that's okay for now.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KryruhdYUr5y"
      },
      "source": [
        "def recover_sentiment_class(sentiment_value: float):\n",
        "  \"\"\"\n",
        "  recovering classes from sentiment_values\n",
        "  [very negative, negative, neutral, positive, very positive]\n",
        "  [0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\n",
        "  [1, 2, 3, 4, 5]\n",
        "  respectively\n",
        "  :params:\n",
        "    sentiment_value: floating value of sentiment\n",
        "  \"\"\"\n",
        "  if sentiment_value <= 0.2:\n",
        "    return 1\n",
        "  elif sentiment_value <= 0.4:\n",
        "    return 2\n",
        "  elif sentiment_value <= 0.6:\n",
        "    return 3\n",
        "  elif sentiment_value <= 0.8:\n",
        "    return 4\n",
        "  else:\n",
        "    return 5"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_SD0R93U51s"
      },
      "source": [
        "dataset_sentiment_df['sentiment_class'] = dataset_sentiment_df['sentiment_values'].apply(\n",
        "    lambda x: recover_sentiment_class(float(x)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "CmU5J0sefMJz",
        "outputId": "e001fe71-ef28-4716-c1f5-560c735d8b8f"
      },
      "source": [
        "dataset_sentiment_df.tail()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase_text</th>\n",
              "      <th>phrase_ids</th>\n",
              "      <th>sentiment_values</th>\n",
              "      <th>sentiment_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>239226</th>\n",
              "      <td>zoning ordinances to protect your community fr...</td>\n",
              "      <td>220441</td>\n",
              "      <td>0.13889</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239227</th>\n",
              "      <td>zzzzzzzzz</td>\n",
              "      <td>179256</td>\n",
              "      <td>0.19444</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239228</th>\n",
              "      <td>élan</td>\n",
              "      <td>220442</td>\n",
              "      <td>0.51389</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239229</th>\n",
              "      <td>É</td>\n",
              "      <td>220443</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239230</th>\n",
              "      <td>É um passatempo descompromissado</td>\n",
              "      <td>220444</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              phrase_text  ... sentiment_class\n",
              "239226  zoning ordinances to protect your community fr...  ...               1\n",
              "239227                                          zzzzzzzzz  ...               1\n",
              "239228                                               élan  ...               3\n",
              "239229                                                  É  ...               3\n",
              "239230                   É um passatempo descompromissado  ...               3\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoMMQF2c-mK2"
      },
      "source": [
        "## 2. Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Op-SSToHIW2"
      },
      "source": [
        "# let's setup the tokenizer\n",
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvDIYbHpIzZ3"
      },
      "source": [
        "# define the vocab and tokenizer from the bert_layer here\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bQ-cgqcI3is"
      },
      "source": [
        "# simple function to encode the sentence\n",
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e64qjpT6IM5q"
      },
      "source": [
        "*we're using bert layer for tokenization only!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7u6v67jIQ5Z",
        "outputId": "85085598-4b99-4cb6-88fc-acf4f75ab5d1"
      },
      "source": [
        "tokenizer.tokenize(\"don't be so judgemental\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['don', \"'\", 't', 'be', 'so', 'judgement', '##al']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Lrf7r0JuNN"
      },
      "source": [
        "*masked language model tokenizer, email me for any queries*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwLJZdEmJWbo"
      },
      "source": [
        "list_of_pharses = list(dataset_sentiment_df['phrase_text'])\n",
        "encoded_phrases = [encode_sentence(phrase) for phrase in list_of_pharses]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVB_bdXWUCGz"
      },
      "source": [
        "y = dataset_sentiment_df['sentiment_class']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLqSt25xUTeN"
      },
      "source": [
        "list_of_pharses_with_length = [[phrase, y[i], len(phrase)]\n",
        "                 for i, phrase in enumerate(encoded_phrases)]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0_7FJdvetoj"
      },
      "source": [
        "random.shuffle(list_of_pharses_with_length)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg7jUDXhgFBI"
      },
      "source": [
        "*shuffling, something we should always do for better tangling*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mb38akef1y0"
      },
      "source": [
        "list_of_pharses_with_length.sort(key=lambda x: x[2])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W42irrBgUio"
      },
      "source": [
        "*to handle the dimension for each sequence model, we pad the sequence as per batch size.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVQpMt2GgV_z"
      },
      "source": [
        "sorted_phrases_sentiments = [(phrase_with_length[0], phrase_with_length[1]) for phrase_with_length in list_of_pharses_with_length]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXJ3ZbH1gpPc"
      },
      "source": [
        "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_phrases_sentiments, output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0jSB575g-_j"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHYKzbvghAxU",
        "outputId": "be29d868-ea88-46e2-eb4e-c3f644ce8807"
      },
      "source": [
        "next(iter(batched_dataset))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(32, 1), dtype=int32, numpy=\n",
              " array([[21962],\n",
              "        [ 3581],\n",
              "        [10657],\n",
              "        [ 2447],\n",
              "        [14474],\n",
              "        [20437],\n",
              "        [10349],\n",
              "        [13999],\n",
              "        [ 4348],\n",
              "        [ 9556],\n",
              "        [22570],\n",
              "        [ 5623],\n",
              "        [ 5593],\n",
              "        [ 4816],\n",
              "        [18006],\n",
              "        [ 6703],\n",
              "        [13827],\n",
              "        [ 5070],\n",
              "        [ 9010],\n",
              "        [12401],\n",
              "        [ 4975],\n",
              "        [ 6644],\n",
              "        [ 8011],\n",
              "        [16839],\n",
              "        [ 2440],\n",
              "        [ 2203],\n",
              "        [ 3458],\n",
              "        [29387],\n",
              "        [ 2789],\n",
              "        [11255],\n",
              "        [19910],\n",
              "        [11245]], dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              " array([3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 4, 2, 2, 3, 3, 3, 3, 3,\n",
              "        2, 3, 4, 3, 3, 3, 3, 3, 3, 3], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0tPZXajhN5B"
      },
      "source": [
        "TOTAL_BATCHES = math.ceil(len(list_of_pharses_with_length) / BATCH_SIZE)\n",
        "TEST_BATCHES = TOTAL_BATCHES // 10\n",
        "batched_dataset.shuffle(TOTAL_BATCHES)\n",
        "test_data = batched_dataset.take(TEST_BATCHES)\n",
        "train_data = batched_dataset.skip(TEST_BATCHES)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ744VqbhV8-"
      },
      "source": [
        "*keeping the 10% of the batched dataset for evaluation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmcnznIDsZ4q"
      },
      "source": [
        "## 3. Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK_nroWFhbii"
      },
      "source": [
        "class FiftyGram_SentimentClassification_VanillaModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 embedding_dimensions=128,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"vanilla_model\"):\n",
        "        super(FiftyGram_SentimentClassification_VanillaModel, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocabulary_size,\n",
        "                                          embedding_dimensions)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=2,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=3,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=4,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if model_output_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=model_output_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        l = self.embedding(inputs)\n",
        "        l_1 = self.cnn_layer1(l) \n",
        "        l_1 = self.pool(l_1) \n",
        "        l_2 = self.cnn_layer2(l) \n",
        "        l_2 = self.pool(l_2)\n",
        "        l_3 = self.cnn_layer3(l)\n",
        "        l_3 = self.pool(l_3) \n",
        "        \n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training)\n",
        "        model_output = self.last_dense(concatenated)\n",
        "        \n",
        "        return model_output"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9mYXzPYlsl4"
      },
      "source": [
        "# Defining all the hyperparameters\n",
        "VOCAB_LENGTH = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "CNN_FILTERS = 100\n",
        "DNN_UNITS = 256\n",
        "OUTPUT_CLASSES = 5\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 4"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2cUvEnMl1fs"
      },
      "source": [
        "vanilla_model = FiftyGram_SentimentClassification_VanillaModel(\n",
        "    vocabulary_size=VOCAB_LENGTH,\n",
        "    embedding_dimensions=EMB_DIM,\n",
        "    cnn_filters=CNN_FILTERS,\n",
        "    dnn_units=DNN_UNITS,\n",
        "    model_output_classes=OUTPUT_CLASSES,\n",
        "    dropout_rate=DROPOUT_RATE\n",
        "    )"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDRWjmJTl-4J"
      },
      "source": [
        "# loss and optimization metrics\n",
        "if OUTPUT_CLASSES == 2:\n",
        "    vanilla_model.compile(loss=\"binary_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "else:\n",
        "    vanilla_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTB88PF0se0i"
      },
      "source": [
        "*model has not been fit yet due to improper embedding dimensions, implemention of keras tuner for better hyperparameters is to be followed*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "329dTBJBmbGi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}